{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "duration": "1.5 hours",
        "id": "d9tB2hJpEbLQ"
      },
      "source": [
        "# Machine Learning with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "min9wx2BEbLS"
      },
      "source": [
        "## What Is Machine Learning?\n",
        "\n",
        "> **\"If you torture the data enough, nature will always confess.\"** –Ronald Coase\n",
        "\n",
        "As a one line version—not entirely original—I like to think of machine learning as \"statistics on steroids.\"  That characterization may be more cute than is necessary, but it is a good start.  Others have used phrases like \"extracting knowledge from raw data by computational means.\"\n",
        "\n",
        "The lede on the Wikipedia article provides a bit more.\n",
        "\n",
        "![Wikipedia entry](https://github.com/bibekebib/ML-Webinar/blob/main/img/ML-Wikipedia.png?raw=1)\n",
        "\n",
        "Cite: [Wikipedia, 09:29, 2018 October 4](https://en.wikipedia.org/w/index.php?title=Machine_learning&oldid=862453222)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised and Unsupervised Model\n",
        "\n",
        "\n",
        "## Supervised Model\n",
        "Supervised learning, as the name indicates, has the presence of a supervisor as a teacher. Supervised learning is when we teach or train the machine using data that is well-labelled. Which means some data is already tagged with the correct answer. After that, the machine is provided with a new set of examples(data) so that the supervised learning algorithm analyses the training data(set of training examples) and produces a correct outcome from labeled data.\n",
        "\n",
        "## Unsupervised Model\n",
        "Unsupervised learning is a type of machine learning that learns from unlabeled data. This means that the data does not have any pre-existing labels or categories. The goal of unsupervised learning is to discover patterns and relationships in the data without any explicit guidance."
      ],
      "metadata": {
        "id": "_EUWa6QqJO0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.scribbr.com/wp-content/uploads/2023/08/supervised-vs-unsupervised-learning-1.webp)"
      ],
      "metadata": {
        "id": "UbhU0l22JjGX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQMlEapJEbLe"
      },
      "source": [
        "## Machine Learning Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SNJPuSjEbLf"
      },
      "source": [
        "## What Is scikit-learn?\n",
        "\n",
        "Scikit-learn provides a large range of algorithms in machine learning that are unified under a common and intuitive API. Most of the dozens of classes provided for various kinds of models share the large majority of the same calling interface. Very often—as we will see in examples below—you can easily substitute one algorithm for another with nearly no change in your underlying code. This allows you to explore the problem space quickly, and often arrive at an optimal, or at least satisficing$^1$ approach to your problem domain or datasets.\n",
        "\n",
        "* Simple and efficient tools for data mining and data analysis\n",
        "* Accessible to everybody, and reusable in various contexts\n",
        "* Built on NumPy, SciPy, and matplotlib\n",
        "* Open source, commercially usable - BSD license\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<small>$^1$<i>Satisficing is a decision-making strategy of searching through the alternatives until an acceptability threshold is met. It is a portmanteau of satisfy and suffice, and was introduced by Herbert A. Simon in 1956. He maintained that many natural problems are characterized by computational intractability or a lack of information, both of which preclude the use of mathematical optimization procedures.</i></small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAvrKrHuEbLg"
      },
      "source": [
        "## Overview of Techniques Used in Machine Learning\n",
        "\n",
        "The diagram below is from the scikit-learn documentation, but the same general schematic of different techniques and algorithms that it outlines applies equally to any other library.  The classes represented in bubbles mostly will have equivalent versions in other libraries.\n",
        "\n",
        "![Scikit-learn topic areas](https://github.com/bibekebib/ML-Webinar/blob/main/img/sklearn-topics.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tEHLR_QEbLh"
      },
      "source": [
        "## Classification versus Regression\n",
        "\n",
        "### Classification\n",
        "\n",
        "Classification is a type of supervised learning in which the targets for a prediction are a set of categorical values.\n",
        "\n",
        "### Regression\n",
        "\n",
        "Regression is a type of supervised learning in which the targets for a prediction are quantitative or continuous values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBoMqUTeEbLh"
      },
      "source": [
        "## Overfitting and Underfitting\n",
        "\n",
        "In machine learning models, we have to worry about twin concerns.  On the one hand, we might **overfit** our model to the dataset we have available.  If we train a model extremely accurately against the data itself, metrics we use for the quality of the model will probably show high values.  However, in this scenario, the model is unlikely to extend well to novel data, which is usually the entire point of developing a model and making predictions.  By training in a fine tuned way against one dataset, we might have done nothing more than memorize that collection of values; or at least memorize a spurious pattern that exists in that particular sample data collection.\n",
        "\n",
        "To some extent (but not completely), overfitting is mitigated by larger dataset sizes.\n",
        "\n",
        "In contrast, if we choose a model that simply does not have the degree of detail necessary to represent the underlying real-world phenomenon, we get an **underfit** model.  In this scenario, we *smooth too much* in our simplification of the data into a model.\n",
        "\n",
        "Some illustrations are useful."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://docs.aws.amazon.com/images/machine-learning/latest/dg/images/mlconcepts_image5.png)\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1396/1*lARssDbZVTvk4S-Dk1g-eA.png)"
      ],
      "metadata": {
        "id": "IIqzclHgE_wt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb6HC3qJEbLk"
      },
      "source": [
        "# Bias and Variance\n",
        "\n",
        "## Bias\n",
        "Bias is simply defined as the inability of the model because of that there is some difference or error occurring between the model’s predicted value and the actual value. These differences between actual or expected values and the predicted values are known as error or bias error or error due to bias.\n",
        "\n",
        "- Low Bias: Low bias value means fewer assumptions are taken to build the target function. In this case, the model will closely match the training dataset.\n",
        "- High Bias: High bias value means more assumptions are taken to build the target function. In this case, the model will not match the training dataset closely.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:978/1*CgIdnlB6JK8orFKPXpc7Rg.png)\n",
        "\n",
        "## Variance\n",
        "Variance is the measure of spread in data from its mean position. In machine learning variance is the amount by which the performance of a predictive model changes when it is trained on different subsets of the training data. More specifically, variance is the variability of the model that how much it is sensitive to another subset of the training dataset. i.e. how much it can adjust on the new subset of the training dataset.\n",
        "\n",
        "- Low variance: Low variance means that the model is less sensitive to changes in the training data and can produce consistent estimates of the target function with different subsets of data from the same distribution. This is the case of underfitting when the model fails to generalize on both training and test data.\n",
        "- High variance: High variance means that the model is very sensitive to changes in the training data and can result in significant changes in the estimate of the target function when trained on different subsets of data from the same distribution. This is the case of overfitting when the model performs well on the training data but poorly on new, unseen test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avAw2q6-EbLm"
      },
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "Dimensionality reduction is most often a technique used to assist with other techniques. By reducing a large number of features to relatively few features; very often other techniques are more successful relative to these transformed synthetic features. Sometimes the dimensionality reduction itself is sufficient to identify the \"main gist\" of your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRJVzVPMEbLm"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Very often, the \"features\" we are given in our original data are not those that will prove most useful in our final analysis. It is often necessary to identify \"the data inside the data.\" Sometimes feature engineering can be as simple as normalizing the distribution of values. Other times it can involve creating synthetic features out of two or more raw features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjAa0OdWEbLn"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "Often, the features you have in your raw data contain some features with little to no predictive or analytic value. Identifying and excluding irrelevant features often improves the quality of a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR6KSzevEbLo"
      },
      "source": [
        "## One-hot Encoding\n",
        "\n",
        "For many machine learning algorithms, including neural networks, it is more useful to have a categorical feature with N possible values encoded as N features, each taking a binary value. Several tools, including a couple functions in scikit-learn will transform raw datasets into this format. Obviously, by encoding this way, dimensionality is increased.\n",
        "\n",
        "Let us illustrate using a toy test dataset.  The following whimsical data is suggested in a blog post by [Håkon Hapnes Strand](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science).  Imagine we collected some data on individual organisms—namely taxonomic class, height, and lifespan.  Depending on our purpose, we might use this data for either supervised or unsupervised learning techniques (if we had a lot more observations, and a number more features)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7Jqj6b8aMkzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'ID': [1, 2, 3, 4, 5],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
        "    'Age': [25, 30, 22, 35, 28],\n",
        "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Green']\n",
        "}\n",
        "\n",
        "# Creating a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Displaying the original DataFrame\n",
        "print(\"DataFrame:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsMwVlquL8ow",
        "outputId": "aa07fab8-0405-457b-89f0-d22a41ef911f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame:\n",
            "   ID     Name  Age  Color\n",
            "0   1    Alice   25    Red\n",
            "1   2      Bob   30   Blue\n",
            "2   3  Charlie   22  Green\n",
            "3   4    David   35    Red\n",
            "4   5      Eva   28  Green\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "color_mapping = {'Red': 'Color_Red', 'Blue': 'Color_Blue', 'Green': 'Color_Green'}\n",
        "df_encoded = pd.concat([df, pd.get_dummies(df['Color'].map(color_mapping))], axis=1)\n",
        "\n",
        "# Dropping the original 'Color' column\n",
        "df_encoded = df_encoded.drop(['Color'], axis=1)\n",
        "\n",
        "# Displaying the DataFrame after manual one-hot encoding\n",
        "print(\"\\nDataFrame after Manual One-Hot Encoding:\")\n",
        "print(df_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Acz7yKqMATV",
        "outputId": "7e5bb10e-a5d2-4e0f-de68-73e74349457c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after Manual One-Hot Encoding:\n",
            "   ID     Name  Age  Color_Blue  Color_Green  Color_Red\n",
            "0   1    Alice   25           0            0          1\n",
            "1   2      Bob   30           1            0          0\n",
            "2   3  Charlie   22           0            1          0\n",
            "3   4    David   35           0            0          1\n",
            "4   5      Eva   28           0            1          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSjS6gLqEbLu"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "After you have trained a model, the big question is \"how good\" is the model.  There is a lot of nuance to answering that question, and correspondingly a large number of measures and techniques.\n",
        "\n",
        "One common technique to look at a combination of successes and failure in a machine learning model is a *confusion matrix*.  Let us look at an example, picking up the whimsical data used above.  Suppose we wanted to guess the taxonomic class of an observed organism and our model had these results:\n",
        "\n",
        "| Predict/Actual | Human    | Octopus  | Penguin  |\n",
        "|----------------|----------|----------|----------|\n",
        "| Human          |  **5**   |    0     |    2     |\n",
        "| Octopus        |    3     |  **3**   |    3     |\n",
        "| Penguin        |    0     |    1     |  **11**  |\n",
        "\n",
        "Giving a single number to describe *how good* the model is is not immediately obvious.  The model is very good at predicting penguins, but it gets rather bad when it predicts octopi.  In fact, if the model predicts something is an octopus, it probably isn't (only 1/3rd of such predictions are accurate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo5zcjysEbLu"
      },
      "source": [
        "### Accuracy versus Precision versus Recall\n",
        "\n",
        "Naïvely, we might simply ask about the \"accuracy\" of a model (at least for classification tasks).  This is simply the number of *right* answers divided by the number of data points.  In our example, we have 28 observations of organisms, and 19 were classified accurately, so that's a **68%** accuracy.  Again though, the accuracy varies quite a lot if we restrict it to just one class of the predictions.  For our multi-class labels, this may not be a bad measure.  \n",
        "\n",
        "Consider a binary problem though:\n",
        "\n",
        "| Predict/Actual | Positive | Negative |\n",
        "|----------------|----------|----------|\n",
        "| Positive       |    1     |    0     |\n",
        "| Negative       |    2     |   997    |\n",
        "\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSdsPFoxS_ROjqfH6LmpP--isjdT4iJKPljE2Q2JLNw_g&s)\n",
        "\n",
        "Calculating *accuracy*, we find that this model is **99.8%** accurate! That seems pretty good until you think of this test as a medical screening for a fatal disease.  *Two thirds of the people who actually have the disease will be judged free of it by this model* (and hence perhaps not be treated for the condition); that isn't such a happy real-world result.\n",
        "\n",
        "<hr/>\n",
        "\n",
        "In contrast with accuracy, the \"precision\" of a model is defined as:\n",
        "\n",
        "$$\\text{Precision} = \\frac{true\\: positive}{true\\: positive + false\\: positive}$$\n",
        "\n",
        "Generalizing that to the multi-class case, the formula is as follows (for i being the index of the class):\n",
        "\n",
        "$$\\text{Precision}_{i} = \\cfrac{M_{ii}}{\\sum_i M_{ij}}$$\n",
        "\n",
        "Applying that to our hypothetical medical screening, we get a a precision of **1.0**.  We cannot do better than that.  The problem is with \"recall\" which is defined as:\n",
        "\n",
        "$$\\text{Recall} = \\frac{true\\: positive}{true\\: positive + false\\: negative}$$\n",
        "\n",
        "Generalizing that to the multi-class case:\n",
        "\n",
        "$$\\text{Recall}_{i} = \\cfrac{M_{ii}}{\\sum_j M_{ij}}$$\n",
        "\n",
        "Here we do much worse by having a recall of **33.3%** in our medical diagnosis case! This is obviously a terrible result if we care about recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSz0vQ8KEbLv"
      },
      "source": [
        "### F1 Score\n",
        "\n",
        "There are several different algorithms that attempt to *blend* precision and recall to product a single \"score.\"  Scikit-learn provides a number of other scalar scores that are useful for differing purposes (and other libraries are similar), but F1 score is one that is used very frequently.  It is simply:\n",
        "\n",
        "$$\\text{F1} = 2 \\times \\cfrac{precision \\times recall}{precision + recall}$$\n",
        "\n",
        "Applying that to our medical diagnostic model, we get an F1 score of 50%.  Still not good, but we account for the high precision to some extent.  For intermediate cases, the F1 score provides good balance.\n",
        "\n",
        "F1 score can be generalized to multi-class models by averaging the F1 score across each class, counting only correct/incorrect per class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySLOlVSJEbLv"
      },
      "source": [
        "### Code Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOEPF9RKEbLw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "y_true = [\"human\",   \"octopus\", \"human\", \"human\", \"octopus\", \"penguin\", \"penguin\"]\n",
        "y_pred = [\"octopus\", \"octopus\", \"human\", \"human\", \"octopus\", \"human\",   \"penguin\"]\n",
        "labels = ['octopus', 'penguin', 'human']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uruSZBzHEbLw",
        "outputId": "548d4944-1c96-49e3-a706-3e7333a77850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (predict/actual):\n",
            "         octopus  penguin  human\n",
            "octopus        2        0      0\n",
            "penguin        0        1      1\n",
            "human          1        0      2\n",
            "\n",
            "Recall:\n",
            "octopus    1.000000\n",
            "penguin    0.500000\n",
            "human      0.666667\n",
            "dtype: float64\n",
            "\n",
            "Precision:\n",
            "octopus    0.666667\n",
            "penguin    1.000000\n",
            "human      0.666667\n",
            "dtype: float64\n",
            "\n",
            "Accuracy:\n",
            " 0.7142857142857143\n"
          ]
        }
      ],
      "source": [
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "print(\"Confusion Matrix (predict/actual):\\n\",\n",
        "      pd.DataFrame(cm, index=labels, columns=labels), sep=\"\")\n",
        "\n",
        "recall = np.diag(cm) / np.sum(cm, axis=1)\n",
        "print(\"\\nRecall:\\n\", pd.Series(recall, index=labels), sep=\"\")\n",
        "\n",
        "precision = np.diag(cm) / np.sum(cm, axis=0)\n",
        "print(\"\\nPrecision:\\n\", pd.Series(precision, index=labels), sep=\"\")\n",
        "\n",
        "print(\"\\nAccuracy:\\n\", np.sum(np.diag(cm)) / np.sum(cm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh8bs79cEbLx"
      },
      "source": [
        "In this particular case, F1 score is very close to accuracy.  In fact, using the \"micro\" averaging method reduces the result to accuracy.  Using the \"macro\" averaging makes it equivalent to a NumPy reduction from the formula given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT6JaX3dEbLx",
        "outputId": "0d0eaddb-8469-4860-9276-18f4205eac98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "F1 score:\n",
            "0.7047619047619048\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "print(\"\\nF1 score:\\n\", weighted_f1, sep=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "?f1_score"
      ],
      "metadata": {
        "id": "Jc-X9czIZkpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPipg7HgEbLx",
        "outputId": "33eecce9-39d9-4d8f-895d-bea7b88aca9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive averaging F1 score: 0.7111111111111111\n",
            " sklearn macro averaging: 0.7111111111111111\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive averaging F1 score:\", np.mean(2*(recall*precision)/(recall+precision)))\n",
        "print(\" sklearn macro averaging:\", f1_score(y_true, y_pred, average=\"macro\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqSVqYffEbLx"
      },
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}